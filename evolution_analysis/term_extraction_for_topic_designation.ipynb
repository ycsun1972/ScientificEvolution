{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('../56529_WoS_Paper.xlsx',sheet_name='TI')\n",
    "corpus_for_word2vec=data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus_for_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    stopwords = []\n",
    "    with open('../Word_List/stopword_list.txt','r',encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.replace('\\n','')\n",
    "            stopwords.append(line)\n",
    "    return stopwords\n",
    "stopwords = get_stopwords()\n",
    "\n",
    "def get_academic_words():\n",
    "    academic_words = []\n",
    "    with open('../Word_List/academic_word_list-570.txt','r',encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.replace('\\n','')\n",
    "            academic_words.append(line)\n",
    "    return academic_words\n",
    "academic_words = get_academic_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def NLP_Prpcessing(text):\n",
    "    text = str(text)\n",
    "    # Remove non-English characters.\n",
    "    text = re.sub(r'[^1-9a-zA-Z_]', \" \", text)\n",
    "    # Remove redundant spaces.\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "\n",
    "    # Convert uppercase to lowercase and split words.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')         \n",
    "    text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Remove stop words and common academic words.\n",
    "    stopword = get_stopwords()  # Load stop word list.\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    academic_word = get_academic_words()  # Load common academic word list.\n",
    "    text = [word for word in text if word not in academic_word]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    text = [token for token in text if not token.isnumeric()]\n",
    "    text = [token for token in text if len(token) > 1]\n",
    "    \n",
    "    # Lemmatize the documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    '''\n",
    "    text = [lemmatizer.lemmatize(token, pos = 'v') for token in text]\n",
    "    text = [lemmatizer.lemmatize(token, pos = 'a') for token in text]\n",
    "    '''\n",
    "    text = [lemmatizer.lemmatize(token, pos = 'n') for token in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus_for_word2vec)):\n",
    "    corpus_for_word2vec[i] = NLP_Prpcessing(corpus_for_word2vec[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "bigram = Phrases(corpus_for_word2vec, min_count = 30, threshold=2)  \n",
    "#trigram = Phrases(bigram[corpus_for_word2vec],min_count = 5, threshold=5)\n",
    "#Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = Phraser(bigram)\n",
    "#trigram_mod = Phraser(trigram)\n",
    "\n",
    "for i in range(len(corpus_for_word2vec)):\n",
    "    corpus_for_word2vec[i] = bigram_mod[corpus_for_word2vec[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus_for_word2vec))\n",
    "print(corpus_for_word2vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "word2vec_model = word2vec.Word2Vec(sentences = corpus_for_word2vec, size=300, min_count = 1,iter=300)\n",
    "word_list = word2vec_model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec_model.wv.similarity('deep_learning','machine_learning'))\n",
    "print(word2vec_model.wv.similarity('face','machine_learning'))\n",
    "print(word2vec_model.wv.similarity('face','facial'))\n",
    "word2vec_model.wv.most_similar('facial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "import gensim\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus('TI&AB.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_corpus))\n",
    "#train_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(dm=0, vector_size = 300, epochs = 10)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples = model.corpus_count, epochs = model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model.save('doc2vecModel.vec')\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "model = Doc2Vec.load('doc2vecModel.vec')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvecs = model.docvecs\n",
    "docvectors = []\n",
    "#for i in range(len(docvecs)):\n",
    "#[0,2753), [2753,6225), [6225,10137), [10137,14675), [14675,19878), [19878,25376), [25376,31657), [31657,38877), [38877,47540), [47540,56529)\n",
    "for i in range(47540,56529):\n",
    "    docvectors.append(docvecs[i])\n",
    "len(docvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "classNumber = 24\n",
    "# k = 12, 14, 14, 16, 17, 17, 19, 21, 23, 24\n",
    "# k = 8, 9, 10, 11, 12, 12, 13, 14, 16, 16\n",
    "kmean_model = KMeans(n_clusters = classNumber).fit(docvectors)\n",
    "labels = kmean_model.labels_\n",
    "cluster_centers = kmean_model.cluster_centers_\n",
    "print(labels)\n",
    "center_dict = Counter(labels)\n",
    "#center_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_dict\n",
    "topicSizeList = []\n",
    "for key, value in sorted(center_dict.items(),key = lambda x:x[0],reverse = False): # Sort by Topic Tags\n",
    "    topicSizeList.append(value)\n",
    "print(topicSizeList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doccluster = [[]*classNumber for x in range(classNumber)]\n",
    "for i in range(0, classNumber):\n",
    "    for j in range(0, len(labels)):\n",
    "        if i == labels[j]:\n",
    "            doccluster[i].append(j+1+47540)\n",
    "            # 0,2753,6225,10137,14675,19878,25376,31657,38877,47540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate topic vectors\n",
    "import numpy as np\n",
    "topicvector = [[]*classNumber for x in range(classNumber)]\n",
    "for m in range(len(doccluster)):\n",
    "    topicvector[m] = [0 for x in range(300)]\n",
    "    for n in range(len(doccluster[m])):\n",
    "        topicvector[m] = np.vstack((topicvector[m],docvecs[doccluster[m][n]-1]))\n",
    "        topicvector[m] = np.mean(topicvector[m],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doccluster[0][:5])\n",
    "len(doccluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the term composition of each topic\n",
    "# topics: semantic network, c_topics: co-occurrence network\n",
    "topics = [[]*classNumber for x in range(classNumber)]\n",
    "c_topics = [[]*classNumber for x in range(classNumber)]\n",
    "for i in range(len(doccluster)):\n",
    "    for j in doccluster[i]:\n",
    "        topics[i] += corpus_for_word2vec[j-1]\n",
    "        c_topics[i].append(corpus_for_word2vec[j-1])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_commonwords():\n",
    "    commonwords = []\n",
    "    with open('CV_commonwords.txt','r',encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.replace('\\n','')\n",
    "            commonwords.append(line)\n",
    "    return commonwords\n",
    "commonwords = get_commonwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency_count(text):\n",
    "    count_dict = {}\n",
    "    # Count the number of each term\n",
    "    text = [token for token in text if (token !='' and token not in commonwords)]\n",
    "    for i in text:\n",
    "        count_dict[i] = count_dict.get(i,0) + 1\n",
    "    \n",
    "    # Convert to a list\n",
    "    count_dict = list(count_dict.items())\n",
    "    count_dict.sort(key = lambda x:x[1], reverse = True)\n",
    "   \n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_frequency_count(topics[1])  #[('deep_learning', 52), ('recognition', 39), ('video', 32)...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct term co-occurrence matrix\n",
    "def kwCoOccurrenceMatrix(terms, c_topics):\n",
    "    edge = len(terms)       # The size of the matrix are the length of the term set\n",
    "    matrix = np.zeros((edge, edge), dtype=str)\n",
    "    appeardict = {}\n",
    "    for i in terms:\n",
    "        appearlist = []\n",
    "        k = 0\n",
    "        for j in c_topics:\n",
    "            if i in j:\n",
    "                appearlist.append(k)\n",
    "            k += 1\n",
    "        appeardict[i] = appearlist\n",
    "    #return appeardict\n",
    "\n",
    "    for row in range(len(terms)):\n",
    "        # Iterate through the first row of the matrix, skipping the elements with subscript 0\n",
    "        for col in range(len(terms)):\n",
    "            # Iterate through the first column of the matrix, skipping the elements with subscript 0\n",
    "            if col >= row:\n",
    "                # Calculate only the upper half of the matrix\n",
    "                if terms[row] == terms[col]:\n",
    "                    matrix[col][row] = 0\n",
    "                else:\n",
    "                    counter = len(set(appeardict[terms[row]])&set(appeardict[terms[col]]))\n",
    "                    matrix[col][row] = counter\n",
    "            else:\n",
    "                matrix[col][row]=matrix[row][col]\n",
    "    return matrix            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from openpyxl import Workbook\n",
    "wb = Workbook()\n",
    "topicSheet_kwDegreeCentrality = wb.create_sheet('topics_kwSemanticNet_Centrality',index=0)\n",
    "topicSheet_kwCoOccurrence = wb.create_sheet('topics_kwCoOccurNet_Centrality',index=1)\n",
    "topicSheet_kw_sheet = wb.create_sheet('topics_kw',index=2)\n",
    "topic_vector_sheet = wb.create_sheet('topic vector', index=3)\n",
    "#topicSheet_kwFre = wb.create_sheet('topics_kwFre',index=4)\n",
    "#topicSheet_kwIntegratedCentrality = wb.create_sheet('topics_Integrated_Centrality', index=2)\n",
    "\n",
    "topn = 30    # Consider only the top 30 keywords in terms of word frequency\n",
    "\n",
    "for i in range(classNumber):\n",
    "    # For each topic, construct term network\n",
    "    termNetwork = nx.Graph()\n",
    "\n",
    "    # Create Node\n",
    "    term = word_frequency_count(topics[i]) \n",
    "    terms = []\n",
    "    termsWithFre = []\n",
    "    for m in range(topn):\n",
    "        if term[m][0] not in commonwords:\n",
    "            terms.append(term[m][0])   # Store nodes\n",
    "            termsWithFre.append(str(term[m][1])+'*'+term[m][0])   # Store high frequency terms \n",
    "    termNetwork.add_nodes_from(terms)\n",
    "\n",
    "    # Create edges\n",
    "    edgeList = []\n",
    "    similarityMatrix = np.zeros(shape=(topn,topn))\n",
    "    for m in range(topn):\n",
    "         for n in range(topn):\n",
    "            similarityMatrix[m][n] = 0.5 + 0.5*word2vec_model.wv.similarity(terms[m],terms[n])  # Normalize to [0,1]\n",
    "    upper_quartile = np.quantile(similarityMatrix, 0.75, interpolation = 'higher')      # Prune according to the upper quartile\n",
    "    similarityMatrix[similarityMatrix < upper_quartile] = 0\n",
    "\n",
    "    for m in range(topn):\n",
    "         for n in range(topn):\n",
    "            if 1 > similarityMatrix[m][n] > 0:\n",
    "                edgeList.append((terms[m],terms[n],similarityMatrix[m][n]))\n",
    "    termNetwork.add_weighted_edges_from(edgeList)\n",
    "\n",
    "\n",
    "    for k in range(len(topicvector[i])):\n",
    "        topic_vector_sheet.cell(row=i+1, column=k+1, value = topicvector[i][k])\n",
    "\n",
    "    topicSheet_kwDegreeCentrality.cell(row=i+1,column=1,value='topic '+ str(i+1))\n",
    "    \n",
    "    termsList = []\n",
    "    for key, value in sorted(nx.degree_centrality(termNetwork).items(),key=lambda x:x[1],reverse=True):   \n",
    "    # the effect of betweenness_centrality or eigenvector_centrality is not very good.\n",
    "        termsList.append((str(round(value,2))+'*'+key))\n",
    "    topicSheet_kwDegreeCentrality.cell(row=i+1,column=2,value=', '.join(termsList))    \n",
    "\n",
    "\n",
    "    # For each topic, construct semantic network\n",
    "    c_termsNetwork = nx.Graph()\n",
    "    c_term = word_frequency_count(topics[i])    \n",
    "    c_terms = []\n",
    "     \n",
    "    # Create Node\n",
    "    for m in range(topn):\n",
    "        if c_term[m][0] not in commonwords:\n",
    "            c_terms.append(c_term[m][0])   # Store nodes\n",
    "    c_termsNetwork.add_nodes_from(c_terms)\n",
    "    \n",
    "    # Create edges\n",
    "    c_edgeList = []\n",
    "    kwMatrix = kwCoOccurrenceMatrix(c_terms, c_topics[i])\n",
    "    for m in range(topn):\n",
    "        for n in range(topn):\n",
    "            if int(kwMatrix[m][n])>0:\n",
    "                c_edgeList.append((c_terms[m],c_terms[n],kwMatrix[m][n]))\n",
    "    c_termsNetwork.add_weighted_edges_from(c_edgeList)\n",
    "\n",
    "\n",
    "    topicSheet_kwCoOccurrence.cell(row=i+1,column=1,value='topic '+ str(i+1))\n",
    "    c_termsList = []\n",
    "    for key, value in sorted(nx.degree_centrality(c_termsNetwork).items(),key=lambda x:x[1],reverse=True):   \n",
    "        c_termsList.append((str(round(value,2))+'*'+key))\n",
    "        #topic = ', '.join(c_termsList)\n",
    "    topicSheet_kwCoOccurrence.cell(row=i+1,column=2,value=', '.join(c_termsList))    \n",
    "\n",
    "    '''\n",
    "    # Sum of two degree centrality\n",
    "    termNetworkDict = nx.degree_centrality(termNetwork)\n",
    "    c_termNetworkDict = nx.degree_centrality(c_termsNetwork)\n",
    "    integratedCentralityList = []\n",
    "\n",
    "    for key,value in termNetworkDict.items():\n",
    "        c_termNetworkDict[key] += value\n",
    "    for key, value in sorted(c_termNetworkDict.items(),key=lambda x:x[1],reverse=True):   \n",
    "        integratedCentralityList.append((str(round(value,2))+'*'+key))\n",
    "    '''\n",
    "\n",
    "    topicSheet_kw_sheet.cell(row=i+1,column=1,value='topic '+ str(i+1))\n",
    "    topicSheet_kw_sheet.cell(row=i+1,column=2,value=', '.join(termsList[:3])) \n",
    "    topicSheet_kw_sheet.cell(row=i+1,column=3,value=', '.join(c_termsList[:3]))  \n",
    "\n",
    "wb.save('TOPICS_termDegree&Fre.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termNetwork.get_edge_data('cnn','object_detection')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02580f3b40faffdca494a56716cab34cc74dfb88737f96a81a04d7c61a2f2be5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
