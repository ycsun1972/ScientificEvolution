{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***Data\n",
    "# 50000_WoS.txt\n",
    "# 50000_MedLine.txt\n",
    "with open('../datasets/50000_WoS.txt','r',encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "docs = list(lines)\n",
    "print(len(docs))\n",
    "print(docs[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***Pre-process and vectorize the documents\n",
    "def remove_stopword():\n",
    "    stopword = []\n",
    "    with open('../wordList/stopword_list.txt','r',encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.replace('\\n','')\n",
    "            stopword.append(line)\n",
    "    return stopword\n",
    "\n",
    "def remove_academic_word():\n",
    "    academic_word = []\n",
    "    with open('../wordList/academic_word_list-2980.txt','r',encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.replace('\\n','')\n",
    "            academic_word.append(line)\n",
    "    return academic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def NLP_Prpcessing(text):\n",
    "    text = str(text)\n",
    "    # Remove non-English characters\n",
    "    text = re.sub(r'[^a-zA-Z\\']', \" \", text)\n",
    "    # Remove redundant spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # Convert uppercase to lowercase and split words\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')         \n",
    "    text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Remove stop words and common academic words \n",
    "    stopword = remove_stopword()  \n",
    "    text = [word for word in text if word not in stopword]\n",
    "\n",
    "    # Remove words that are only one character\n",
    "    text = [token for token in text if not token.isnumeric()]\n",
    "    text = [token for token in text if len(token) > 1]\n",
    "    # Lemmatize the documents\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #text = [lemmatizer.lemmatize(token, pos = 'v') for token in text]\n",
    "    text = [lemmatizer.lemmatize(token, pos = 'a') for token in text]\n",
    "    text = [lemmatizer.lemmatize(token, pos = 'n') for token in text]\n",
    "\n",
    "    academic_word = remove_academic_word()  \n",
    "    text = [word for word in text if word not in academic_word]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import  tqdm\n",
    "for i in tqdm(range(len(docs))):\n",
    "    docs[i] = NLP_Prpcessing(docs[i])\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*Preserve phrases. When a phrase occurs in more than 20 documents, it will not be split. e.g., machine_learning\n",
    "from gensim.models import Phrases\n",
    "bigram = Phrases(docs, min_count = 20)\n",
    "for idx in tqdm(range(len(docs))):\n",
    "    docs[idx] = bigram[docs[idx]]\n",
    "\n",
    "\n",
    "#* Remove low frequency terms (occurring in less than 2 documents) and high frequency terms (occurring in more than 60% of documents)\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.5)\n",
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Convert text to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "#print(corpus[0])\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))  \n",
    "print('Number of documents: %d' % len(corpus))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "unique_word_list = []\n",
    "for i in range(len(dictionary)):\n",
    "    unique_word_list.append(dictionary[i])\n",
    "f = open(\"unique_word_list.txt\",\"w\")\n",
    "for line in unique_word_list:\n",
    "    f.write(line+'\\n')\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***Training\n",
    "from gensim.models import LdaModel\n",
    "temp = dictionary[0] \n",
    "id2word = dictionary\n",
    "num_topics = 10\n",
    "print('Number of topics: %d' % num_topics)\n",
    "lda_model = LdaModel(corpus = corpus, id2word = id2word, alpha = 'auto', eta = 'auto',\n",
    "                     iterations = 6000, num_topics = num_topics, chunksize = 5000, passes = 40)\n",
    "# , random_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_distribution = lda_model.get_document_topics(bow = corpus)\n",
    "LDAlist = []\n",
    "for documents in document_distribution:\n",
    "    LDAlist.append(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as ny\n",
    "prob = [[]*len(LDAlist) for x in range(len(LDAlist))]\n",
    "prob_max_index = [[]*len(LDAlist) for x in range(len(LDAlist))]\n",
    "doc_labels = []\n",
    "for docid in tqdm(range(len(LDAlist))):\n",
    "    for tid in range(len(LDAlist[docid])):\n",
    "        prob[docid].append(LDAlist[docid][tid][1])\n",
    "    prob_max_index[docid] = prob[docid].index(max(prob[docid]))\n",
    "    doc_labels.append(LDAlist[docid][prob_max_index[docid]][0])    \n",
    "doc_labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_label():\n",
    "    ground_truth_label = []\n",
    "    # 50000_WoS_WC.txt\n",
    "    # 50000_MedLine_Label.txt\n",
    "    with open('../datasets/50000_WoS_Lable.txt','r',encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = int(line.replace('\\n',''))\n",
    "            ground_truth_label.append(line)\n",
    "    return ground_truth_label\n",
    "ground_truth_label = get_ground_truth_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.adjusted_rand_score(doc_labels, ground_truth_label))\n",
    "print(metrics.fowlkes_mallows_score(doc_labels, ground_truth_label))\n",
    "print(metrics.adjusted_mutual_info_score(doc_labels, ground_truth_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a018d3a992d78c184ccc94ab54168c7b26325ed3c2283926339c3edbf5487e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
